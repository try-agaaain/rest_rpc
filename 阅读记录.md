

# 副线任务：理清rest_rpc代码，为这个项目写一份文档

connection类： 用于处理客户连接，当有新的客户连接到达时，处理相关的读/写/异常等事件

io_service_pool类：为监听socket而创建的线程池，里面的socket用于监听新的客户端连接
 + io_service_pool中线程池的数量为cpu的核心数，每个线程负责一个监听socket。如果客户端连接请求不那么频繁，那么实际上每次每次只有一个线程在监听客户端连接，可以由下面这段代码看出：
```
  void do_accept() {
    conn_.reset(new connection(io_service_pool_.get_io_service(),
                               timeout_seconds_, router_));
    ...
  }
```
    在每次接受新的请求时，从io_service池中拿到一个io_service，

在rest_rpc中，可能还有心跳机制，需要留意


问题一：监听socket是多线程的，已连接socket是多线程的吗？当客户端连接数量超过io_service池的容量时，rest_rpc是如何处理这些连接的？
  + rest_rpc::server有一个私有成员 sub_map_，似乎可以管理多个已连接socket？
  + 虽然会每次do_accept的时候都会触发 conn_.reset 函数，但每个conn_都会被放入sub_map_，新的问题是，这些conn_似乎只有最后析构rest_rpc::server的时候才会被释放？
  + 正常情况下，已连接socket是通过析构函数释放的，现在这些已连接socket被存放在sub_map_中，如果conn_不从sub_map_中弹出，那么里面的socket_也不会被释放？
    // conn_中的callback_将会在读写事件后调用，
    // 当请求为普通的远程调用时，并不会调用callback_
    // 当请求为订阅发布时，会调用callback_
  + 对于普通的远程调用，已连接socket会在什么时候释放？分析socket的生命周期（它和conn_的生命周期是一样的）
  + server::clean 函数会不断的清理已断开的连接，此时conn_和socket会被释放

问题二：如果有多个普通的远程调用，它们长期的发送请求，server所能接收的最大客户端连接是多少个？
  + 在完成一次请求后，connection会做什么？
    每接受一个客户端请求，首先read_head，并等待异步的读写事件，当读写事件到来后，通过handler调用read_body。
    在read_body中，首先调用read_head，准备异步等待处理当前客户的下一个请求；随后解析rpc数据并返回客户端请求（有点像echo过程）。
    所以在这个过程中，不断的进行这样的过程：read_head[接收用户数据] -> read_body[处理用户数据并返回请求结果] -> read_head[接收用户数据] -> ...
  + 每次接收一个新的客户端请求时，conn_都会调用reset重置为一个新的connection，在这个过程中，旧的connection会被释放吗？会受到影响吗？
    rest_rpc::server中，有两个私有成员用于保存connection：sub_map_和connections_，前者用于处理订阅发布，后者保存所有的客户端连接
    所以当conn_被重置时，旧的连接不会被释放

问题三：客户端是如何构造rpc请求数据包的？
  + 在const_vars.h 中有rpc_head结构体，定义了rpc包的头部结构：
    struct rpc_header {
        uint8_t magic;
        request_type req_type;
        uint32_t body_len;
        uint64_t req_id;
        uint32_t func_id;
    };
  + 那么rpc的主体是如何构造的？参数是如何传递的？
    在rpc_client.h中能找到相关结构体如下：
    struct client_message_type {
        std::uint64_t req_id;
        request_type req_type;
        string_view content;
        uint32_t func_id;
    };
  + rpc协议有专门的数据包格式，通过类似tlv协议的方式划分不同的包，在构造数据包时，将请求参数转化为一个元组，再通过msgpack进行打包
    template <typename... Args> static buffer_type pack_args(Args &&...args) {
        buffer_type buffer(init_size);
        auto temp = std::forward_as_tuple(std::forward<Args>(args)...);
        msgpack::pack(buffer, temp);
        return buffer;
    }
  + 当客户获取到服务器返回的请求后，还需要手动将数据转化为正确的格式，通过as函数完成，如下所示：
    client.async_call<>(
      "get_person",
      [&client](const asio::error_code &ec, string_view data) {
        if (ec) {
          std::cout << "error code: " << ec << ", err msg: " << data << '\n';
          return;
        }
        auto p = as<person>(data);
        CHECK_EQ(p.id, 1);
        CHECK_EQ(p.age, 20);
        CHECK_EQ(p.name, "tom");
      },
      test);


问题四：rest_rpc中有专门处理粘包问题吗？有采用发送队列的方式确保服务器有序的发送数据吗？
    + 在connection类的read_head和read_body函数中，使用的是asio::async_read函数而不是asio::async_read_some，
      前者可以指定要读取多少字节，而后者读取的数据可能不是完整的数据头部或数据体
      在读取数据的过程中，因为头部的数据长度是固定的HEAD_LEN_大小，所以可以用async_read指定读取HEAD_LEN_长度的数据获得完整的头部信息；
      随后，在头部中有消息体的长度信息body_len，根据这一长度用async_read读取body_len长度的数据，即可获得消息体的信息
      这能很方便的解决粘包问题

    + 在connection类中有专门的发送队列write_queue_用于保证数据的有序发送，这一机制包含在以下三个函数中
      void response_interal(uint64_t req_id, std::shared_ptr<std::string> data,
                        request_type req_type = request_type::req_res); //在这个函数中将待发送的数据加入消息队列
      void write(); // 在这个函数中调用boost::async_write进行实际的写操作，将数据发送给客户端，随后调用on_write善后
      void on_write(asio::error_code ec, std::size_t length);   // 在这个函数中判断发送队列是否为空，不为空则弹出队首元素，继续调用write函数发送数据



问题五：rest_rpc的心跳机制是如何进行的？
    + 在connection的read_head函数中有这样一行注释 // nobody, just head, maybe as heartbeat.
      这表明心跳数据包仅包含头文件


问题六：发布订阅是什么？rest_rpc是如何完成这个功能的？
    + 发布-订阅类似于生活中的主题订阅，比如在bilibili关注了一个up主，当up主发布新的动态时，用户的动态列表中可以看到up主的新动态；
      在这个过程中，up主是发布者，用户是订阅者，他们都属于客户端，服务器则相当于是一个中转站；
      关注同一个up主的用户会获得一个token，服务器将动态转发给有相同token的客户
    + 如何使用发布订阅功能？
      在rpc_client类中，定义和publish和subscribe两个函数，应该是分别用来发布内容和订阅内容的；
      需要两个客户端程序和一个服务器程序；服务器程序将publish注册到



问题七：现有测试有哪些？还需要添加哪些测试？如何计算测试覆盖率？
    + 现有测试如下：
        TEST_CASE("test_client_reconnect") {
        TEST_CASE("test_client_default_constructor") {
        TEST_CASE("test_constructor_with_language") {
        TEST_CASE("test_client_async_connect") {
        TEST_CASE("test_client_sync_call") {
        TEST_CASE("test_client_sync_call_return_void") {
        TEST_CASE("test_client_async_call_empty_obj") {
        TEST_CASE("test_client_async_call") {
        TEST_CASE("test_client_async_call_not_connect") {
        TEST_CASE("test_client_async_call_with_timeout") {
        TEST_CASE("test_client_subscribe") {
        TEST_CASE("test_client_subscribe_not_exist_key") {
        TEST_CASE("test_server_publish_encode_msg") {
        TEST_CASE("test_client_subscribe_by_token") {
        TEST_CASE("test_client_publish_and_subscribe_by_token") {
        TEST_CASE("test_server_callback") {
        TEST_CASE("test_server_user_data") {
        TEST_CASE("test_server_delay_response") {
    其中缺乏ssl相关测试，
  + 测试需要考虑哪些函数是没有被测试到的，函数中的哪些分支是没有被测试到的？
  + 通过lcov工具了解到router中的get_name_by_key以及client_util.h中的get_err_msg两个函数是没有被测试覆盖的，可以针对它们两进行测试
    测试报告的生成：
      先执行测试文件
      cd 进入含有.gcda文件的目录
      lcov --capture --directory . --output-file coverage.info
      genhtml coverage.info --output-directory new_test

  + 对于get_err_msg，要了解msgpack::unpack的用法，它的返回值是什么？

  + 只需要编写一个测试用例，当调用错误的函数名时，会同时触发get_name_by_key和get_err_msg两个函数，在捕获的异常中，会


##副线：制作一个项目文档，让贡献者更好的参与进来，让用户更好的了解项目设计 